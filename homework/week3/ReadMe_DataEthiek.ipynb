{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ethiek\n",
    "\n",
    "Bij het maken van een datamodel kijk je als data scientist/ data modeler altijd wat je insluit en wat je uitsluit aan data. Een model is namelijke en gesimplificeerde vorm van de werkelijkheid. Neem Google Maps, daar zijn wegen in meegenomen, maar gebouwen uitgesloten zodat je het overzicht houdt in je navigatie.\n",
    "\n",
    "In de combinatie van algoritmes en data mining treden vijf mechanismen op die disproportioneel nadelige gevolgen voor bepaalde groepen in de samenleving kunnen opleveren. Er zijn 5 oorzaken van bias:\n",
    "1. Target variables,\n",
    "2. Trainingsdata: garbage in garbage out, \n",
    "3. Feature selection,\n",
    "4. Proxies, \n",
    "5. Masking\n",
    "\n",
    "\n",
    "\n",
    "1. Target Variables\n",
    "Twee begrippen zijn hierbij belangrijk: ‘target variables’ en ‘class labels’ - ook wel ‘doelvariabelen’ en ‘klassen labels’. De gewenste uitkomst, waar men naar op zoek is, wordt de doelvariabele genoemd; neem bijvoorbeeld een ‘goede’ werknemer. De klassen labels verdelen alle mogelijke eigenschappen van de doelvariabele in categorieën. \n",
    "\n",
    "De definitie van een goede werknemer is geen gegeven, hoe definieer je ‘goed’? Met andere woorden: wat zouden de klassen labels moeten zijn?\n",
    "\n",
    "Selectiebeslissingen gemaakt op basis van de voorspelde arbeidsduur hebben meer kans een negatief effect te hebben op bepaalde groepen dan selectie-beslissingen gebaseerd op een voorspelling van productiviteit. Dit werkt als volgt. Wanneer het verloop onder medewerkers in het verleden systematisch hoger was binnen een bepaalde groep, zullen selectie-beslissingen gebaseerd op ‘voorspelde arbeidsduur’ resulteren in minder baankansen voor leden die in deze groep vallen; ook al zouden zij als individu net zo goed, of misschien zelfs beter, gepresteerd hebben dan andere sollicitanten die niet binnen die groep vallen en door het bedrijf uiteindelijk zijn aangenomen.\n",
    "\n",
    "Denk bijvoorbeeld aan het volgende: Het verloop van vrouwen ligt hoger, vanwege zwangerschap en ongelijke wettelijke voorwaarden voor vaderschapsverlof, waardoor mannen door blijven werken. Hierdoor kunnen vrouwen systematisch benadeeld worden in selectiebeslissingen terwijl zij net zo goed, of zelfs beter, zouden hebben gepresteerd als mannen. Bovendien is er een grote groep vrouwen die wél blijft doorwerken. \n",
    "\n",
    "2. Trainingsdata: Garbage in, garbage out\n",
    "Er is een oud gezegde in de computerwetenschappen: ‘garbage in, garbage out’. Het draait er bij ‘labelling examples’ om hoe er voorheen keuzes zijn gemaakt  - indien er eerder foutieve of onbetrouwbare data is gebruikt voor bepaalde beslissingen, zal het algoritme de vooroordelen van deze voorbeelden reproduceren. Data mining beschouwt de trainingsdata tenslotte als waarheid / feiten; het algoritme zal bij onjuiste data resultaten opleveren die onbetrouwbaar, en in het slechtste geval discriminerend, zijn.\n",
    "\n",
    "Onder- en oververtegenwoordiging \n",
    "Wanneer data mining conclusies trekt uit de gegevens van een slecht samengestelde steekproef, dan kan elke beslissing systematisch nadelig zijn voor degenen die onder- of oververtegenwoordigd waren in die steekproef. De gegevens van bepaalde bevolkingsgroepen zijn dan niet accuraat of representatief. Zo kunnen bepaalde groepen of mensen worden ondervertegenwoordigd, of zelfs over het hoofd gezien.\n",
    "\n",
    "Ondervertegenwoordiging is iets waar veel onderzoekers zich zorgen over maken. Het systematische uitsluiten van mensen die leven op de rand van de ‘big data’ samenleving, zorgt ervoor dat hun leven minder ‘datafied’ is dan dat van de rest van de samenleving – er worden simpelweg systematisch minder gegevens over deze groepen verzameld. Dit heeft oorzaken als armoede, geografische ligging en levensstijl. Het gevolg is dat deze groepen systematisch zijn ondervertegenwoordigd. Dit probleem van ondervertegenwoordiging doet zich ook in grote mate voor bij historisch achtergestelde groepen, omdat zij minder betrokken zijn bij de formele economie en ongelijke toegang hebben (gehad) tot technologie, onderwijs of andere faciliteiten. Deze tekortkomingen hebben dus zowel invloed op de kwaliteit van de data als op onder/oververtegenwoordiging van groepen mensen in datasets. Bij het analyseren van zulke slecht samengestelde datasets liggen foutieve conclusies en discriminatie op de loer.   \n",
    "\n",
    "3. Feature selection\n",
    "Door slechts een paar indicatoren te selecteren kan een bias ontstaan en kunnen bepaalde groepen onbedoeld buitengesloten worden. Neem het volgende voorbeeld: kinderopvangtoeslag affaire van de Belastingdienst.\n",
    "\n",
    "\n",
    "4. Proxies\n",
    "Wat zijn proxies? Proxies zijn cijfers die correlaties aanduiden tussen bepaalde concepten, waardoor ze dienen als een soort voorspellers. Blond haar dient bijvoorbeeld als een proxy voor een blank huidtype. Aan de hand van zulke correlaties worden mensen door algoritmes ingedeeld in groepen. \n",
    "\n",
    ">>AMAZONE Automation has been key to Amazon’s e-commerce dominance, be it inside warehouses or driving pricing decisions. The company’s experimental hiring tool used artificial intelligence to give job candidates scores ranging from one to five stars - much like shoppers rate products on Amazon, some of the people said.\n",
    "“Everyone wanted this holy grail,” one of the people said. “They literally wanted it to be an engine where I’m going to give you 100 resumes, it will spit out the top five, and we’ll hire those.”\n",
    "\n",
    "Wat kunnen we eraan doen? Het proxy-probleem blijkt er een die moeilijk is op te lossen. Barocase en Selbst gaven aan dat computerwetenschappers niet weten hoe om te gaan met de ‘redundant encodings’ in datasets. Want door eenvoudigweg deze variabelen uit de datamining-oefening te verwijderen, worden ook vaak de criteria verwijderd die aantoonbare en gerechtvaardigde relevantie hebben. \n",
    "Maar verwijder je ze niet? Dan zullen de algoritmes blijven doordenken en keer op keer nieuwe patronen blijven ontdekken. Op die manier zullen telkens meer verbanden worden gelegd tussen eigenschappen en lidmaatschap van bepaalde groepen. Hoe de algoritmes zulke links leggen is voor het menselijk oog vaak nog moeilijk te volgen. Daarbij wordt dan ook regelmatig gesproken van ‘the black box’. Deze ‘black box’ lijkt momenteel de grootste zorg omtrent de algoritmisering van de samenleving.\n",
    "\n",
    "5. Masking\n",
    "bovenstaande bewust toepassen. (an array of booleans that determines for each element of the associated array whether the value is valid or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
